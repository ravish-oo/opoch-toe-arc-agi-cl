### ðŸ”¹ WO-M5.1 â€“ Result & diagnostics struct âœ… COMPLETE

**Goal:** define a single, structured object that captures *everything* about a solve attempt, especially in failure.

**File:** `src/runners/results.py`

**Scope (high-level):**

* Define something like:

  ```python
  @dataclass
  class SolveDiagnostics:
      task_id: str
      law_config: TaskLawConfig
      status: Literal["ok", "infeasible", "mismatch", "error"]
      solver_status: str                 # from pulp
      num_constraints: int
      num_variables: int
      schema_ids_used: list[str]
      # optional: per-schema constraint counts

      # Only for training tasks:
      train_mismatches: list[dict]       # e.g. [{ "example_idx": 0, "diff_cells": [...] }, ...]

      # Debug:
      error_message: str | None
  ```

* This struct is what the Pi-agent will see:

  * if `status != "ok"`, it gets **explicit reasons**: infeasible, mismatch, where mismatched, etc.

---

### ðŸ”¹ WO-M5.2 â€“ Extend kernel to return diagnostics, not just grids âœ… COMPLETE

**Goal:** make `solve_arc_task` produce **diagnostics + outputs** in a way that is fail-closed and Pi-agent-friendly.

**File:** `src/runners/kernel.py` (augment)

**Scope:**

* Change / wrap `solve_arc_task(task_id, law_config)` to something like:

  ```python
  def solve_arc_task_with_diagnostics(
      task_id: str,
      law_config: TaskLawConfig,
      use_training_labels: bool = False
  ) -> tuple[dict[str, list[Grid]], SolveDiagnostics]:
      """
      Returns:
        outputs: {"train": [...], "test": [...]}
        diagnostics: SolveDiagnostics
      """
  ```

* Behavior:

  * Always:

    * build TaskContext,
    * build constraints via schemas,
    * call solver,
    * decode y â†’ grids,
    * fill `SolveDiagnostics` with:

      * status = "ok" / "infeasible" / "error".
  * If `use_training_labels=True`:

    * compare predicted vs true train outputs,
    * set status = "mismatch" if any differ,
    * populate `train_mismatches` with per-example diff info.

This is exactly the â€œfail-close + intermediate infoâ€ you mentioned.

---

### ðŸ”¹ WO-M5.3 â€“ Training sweep + catalog builder script âœ… COMPLETE

**Goal:** one script that a Pi-agent / human can drive to **try law configs on all training tasks and build/update the Catalog**.

**File:** `src/runners/build_catalog_from_training.py`

**Scope:**

* For each `task_id` in `arc-agi_training_challenges.json`:

  * Load an existing `TaskLawConfig` (if any) from `catalog/store.py` **or** receive one from outside (Pi-agent).
  * Call `solve_arc_task_with_diagnostics(task_id, law_config, use_training_labels=True)`.
  * If `status == "ok"`:

    * mark this config as **valid** for that task,
    * write/update it in the catalog store.
  * If `status == "mismatch"` or `"infeasible"`:

    * log diagnostics to a JSON or log file for that task:

      * mismatches, solver_status, schemas used.
* This script does **no law discovery**; it just:

  * runs the kernel,
  * records successes,
  * outputs failures in a Pi-agent-readable format.

This is the main â€œsweep + recordâ€ entrypoint.

---

### ðŸ”¹ WO-M5.4 â€“ Pi-agent harness / interface âš ï¸ DEFERRED

**Goal:** a thin Python interface that exposes everything a Pi-agent needs via simple function calls, without forcing it to touch internals.

**File:** `src/agents/pi_interface.py`

**Scope:**

* Define functions like:

  ```python
  def load_task_summary(task_id: str) -> dict:
      """
      Returns:
        - basic info about the task,
        - maybe small grid previews,
        - counts of components/colors, etc.
      """

  def try_law_config_on_task(
      task_id: str,
      law_config: TaskLawConfig
  ) -> SolveDiagnostics:
      """
      Runs solve_arc_task_with_diagnostics(use_training_labels=True)
      but only returns diagnostics so Pi-agent can see status & mismatches.
      """

  def save_law_config(task_id: str, law_config: TaskLawConfig) -> None:
      """
      Writes to catalog via catalog.store, for configs Pi-agent believes are good.
      """
  ```

* The idea:

  * Pi-agent (you + LLM in an interactive session) only talk to `pi_interface`:

    * see task summaries,
    * propose/update law configs,
    * get rich diagnostics when it fails.

This layer is where we later plug â€œPi-agent as a prompt/programâ€ without touching the math kernel.

---

### ðŸ”¹ WO-M5.5 â€“ Human/Pi-agent log-friendly formatting

**Goal:** make failure cases easy to read and reason about (for both humans and Pi-agent).

**File:** `src/runners/logging_utils.py`

**Scope:**

* Helpers to pretty-print `SolveDiagnostics`:

  * print which schemas used,
  * print mismatched train examples with side-by-side grids,
  * print counts (constraints, variables).
* Optionally, convert `SolveDiagnostics` into a **compact JSON** that Pi-agent can read and reason about.

This is mostly sugar, but itâ€™s important for your â€œobserver is observedâ€ workflow: we want the system itself to provide introspectable state.

---

## ðŸ”¹ WO-M5.X â€“ Enrich diagnostics with per-schema stats & example summaries âœ… COMPLETE

**Goal:** give the Pi-agent more structured evidence by:

1. Tracking how many constraints each schema family contributed.
2. Providing a compact summary of each training/test example (shapes, components).

**Files touched (high-level):**

* `src/runners/results.py`
* `src/schemas/dispatch.py`
* `src/schemas/context.py` (or a new `summaries.py`)
* `src/runners/kernel.py`

---

### Part A â€“ Per-schema constraint counts

**Idea:** each time we apply a schema instance, record how many constraints it added.

**High-level scope:**

* Extend `SolveDiagnostics` with:

  ```python
  schema_constraint_counts: Dict[str, int]
  ```

* In `apply_schema_instance(...)` (in `dispatch.py`):

  * Capture `before = len(builder.constraints)`.
  * Call the `build_Sk_constraints(...)` function.
  * Capture `after = len(builder.constraints)`.
  * Increment `schema_constraint_counts[family_id] += (after - before)`.

* Ensure `kernel.solve_arc_task_with_diagnostics(...)` passes this dict into `SolveDiagnostics`.

**Outcome:** Pi-agent can see which schemas were *actually active* and how heavily they were used.

---

### Part B â€“ Example-level summary (compact)

**Idea:** each example (train/test) gets a small digest: shape + component stats.

**High-level scope:**

* Define an `ExampleSummary` dataclass, e.g. in `results.py` or a new `summaries.py`:

  ```python
  @dataclass
  class ExampleSummary:
      input_shape: tuple[int, int]
      output_shape: Optional[tuple[int, int]]  # None for test inputs
      components_per_color: Dict[int, int]     # color -> count of components
  ```

* Extend `SolveDiagnostics` with:

  ```python
  example_summaries: List[ExampleSummary]
  ```

* In `kernel.solve_arc_task_with_diagnostics(...)`:

  * For each ExampleContext in `TaskContext.train_examples` and `test_examples`:

    * Compute `input_shape`, `output_shape` from grids.
    * Use `components` to build `components_per_color` (count map).
    * Append an `ExampleSummary` to a list.
  * Pass that list into `SolveDiagnostics`.

**Outcome:** Pi-agent can quickly see:

* per-example sizes,
* whether output shape matches expectation,
* how many components of each color exist.