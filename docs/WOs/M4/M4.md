### üîπ WO-M4.1 ‚Äì LP/ILP solver wrapper ‚úÖ COMPLETE

**File:** `src/solver/lp_solver.py`
**Goal:** turn `ConstraintBuilder.constraints` into an optimization problem, solve for y, and return the y vector.

**Scope:**

* Choose a standard library:

  * Prefer `pulp` or `ortools.linear_solver` (whichever is easier in your environment).

* Define a function like:

  ```python
  def solve_constraints(
      builder: ConstraintBuilder,
      num_pixels: int,
      num_colors: int,
      objective: str = "min_sum"
  ) -> np.ndarray:
      """
      Create binary variables y[p_idx, c], add all LinearConstraints,
      add one-hot constraints if not already included, solve the LP/ILP,
      return a flat numpy array of shape (num_pixels, num_colors) or raise if infeasible.
      """
  ```

* Features to include:

  * All constraints in `builder.constraints` as equality constraints.
  * Variables y[p,c] ‚àà {0,1}.
  * One-hot constraints per pixel (you may already add these in M2; ensure they‚Äôre included exactly once).
  * Simple objective:

    * e.g. minimize sum(y) or just `0` (dummy objective) ‚Äî since TU guarantees a vertex, we just need a feasible solution.

* Constraint validation:

  * If solver reports infeasible or unbounded:

    * raise a clear exception or return a special status for the Pi-agent / runner to handle.

This addresses:

* ‚ÄúConstraint validation‚Äù: solver will detect inconsistencies.
* Prepares for decoding in the next WO.

---

### üîπ WO-M4.2 ‚Äì Solution decoding (y ‚Üí Grid(s)) ‚úÖ COMPLETE

**File:** `src/solver/decoding.py`
**Goal:** given the solved y (one-hot over colors), reconstruct output grid(s).

**Scope:**

* Define decoding helpers:

  ```python
  def y_to_grid(
      y: np.ndarray, 
      H: int, 
      W: int, 
      C: int
  ) -> Grid:
      """
      y: shape (H*W, C) or flat with length H*W*C.
      For each pixel p, pick color c where y[p,c] ‚âà 1.
      """
  ```

* If your B(T) and indexing span **multiple outputs** (e.g. all train+test grids in one system), define a small layout struct:

  ```python
  @dataclass
  class VarLayout:
      num_examples: int
      example_shapes: list[tuple[int,int]]  # [(H1,W1), (H2,W2), ...]
      C: int
      # methods to map (example_idx, r, c, color) <-> global y index
  ```

  And a decoder:

  ```python
  def y_to_grids(y: np.ndarray, layout: VarLayout) -> list[Grid]:
      # split y into per-example grids
  ```

* Use the same indexing conventions as `src/constraints/indexing.py`.

This directly addresses your note: ‚Äúsolution decoding (y ‚Üí Grid) belongs in M4.‚Äù

---

### üîπ WO-M4.3 ‚Äì Integrate solver into kernel runner

**File:** `src/runners/kernel.py` (augment)
**Goal:** go from **task_id + law_config** all the way to **decoded output grids**.

**Scope:**

* Extend `solve_arc_task` (or add a new function) to:

  ```python
  def solve_arc_task(
      task_id: str,
      law_config: TaskLawConfig
  ) -> dict[str, list[Grid]]:
      """
      Returns something like:
      {
        "train_outputs_pred": [...],
        "test_outputs_pred": [...]
      }
      """
  ```

* Steps inside:

  1. Load raw task (using `arc_io`).
  2. Build `TaskContext` (using `context.build_task_context_from_raw`).
  3. Create a fresh `ConstraintBuilder`.
  4. Loop over `law_config.schema_instances`, call `apply_schema_instance(...)`.
  5. Compute layout info (N, C or VarLayout).
  6. Call `solve_constraints(...)` from `lp_solver`.
  7. Decode y ‚Üí output grid(s) using `decoding.y_to_grid` / `y_to_grids`.
  8. Return those grids.

* For now, you can focus on:

  * Just the **test outputs** (predictions),
  * Optionally also **predicted train outputs** for internal validation.

---

### üîπ WO-M4.4 ‚Äì Training-set validation runner (sanity / debug)

**File:** `src/runners/validate_on_training.py`
**Goal:** provide a simple script to check if a given `TaskLawConfig` actually reproduces known training outputs.

**Scope:**

* For a single `task_id`:

  * Load `arc-agi_training_challenges.json` and `arc-agi_training_solutions.json`.
  * Build `TaskContext` and call `solve_arc_task(...)` to get predicted train outputs.
  * Compare predicted vs true train outputs grid-by-grid.
  * Print:

    * exact match / mismatch,
    * maybe mismatched cells for debugging.

* This is the place where, later, a Pi-agent would learn:

  * ‚ÄúMy law_config for this task is wrong‚Äù ‚Üí refine schema/params.

This is not Pi-agent logic yet, just a **human-facing checker**.

---

So M4, at a high level, is:

1. **lp_solver.py** ‚Äì encode `ConstraintBuilder` into LP/ILP and solve.
2. **decoding.py** ‚Äì map solved y back to grid(s).
3. **kernel.py** ‚Äì wire solver + decoding into `solve_arc_task`.
4. **validate_on_training.py** ‚Äì simple validation loop on training outputs.

Once these are in place, we‚Äôll have the full math kernel **closed**: given a task and a law_config, the system can actually produce grids and see if they match training labels. That‚Äôs the platform we need before we bring in a Pi-agent to learn laws.
