## Overall sequence:

> **operators â†’ law families â†’ schema instances â†’ builder functions â†’ ConstraintBuilder â†’ solver â†’ Pi-agent loop**

---

## 1. High-level milestones (sequential)

We donâ€™t implement all now, just to show how operators fit.

1. **Operator / feature library (Ï†)** âœ… COMPLETE  

   * Grid representation, basic utilities.
   * All feature extractors: components, bands, hashes, etc.

2. **Constraint representation & ConstraintBuilder** âœ… COMPLETE 

   * Data structures for linear constraints on y.
   * Primitive methods like `tie_pixel_colors`, `fix_pixel_color`, etc.

3. **Schema family builders (S1â€“S11)** âœ… COMPLETE 

   * For each S_k, a `build_Sk_constraints(...)` that uses operators + ConstraintBuilder.

4. **Solver integration** âœ… COMPLETE 

   * Connect to a standard LP/ILP library (`pulp`, `ortools`, or `cvxpy`).
   * Encode constraints and solve for y.

5. **Task IO + test harness**

   * Load ARC tasks, run full pipeline on training pairs, compare outputs.

6. **Pi-agent orchestration layer**

   * LLM decides which schema families + params to use, calls the pipeline, interprets results.

---

## Milestone 1 â€“ Operator / feature library (Ï†) âœ… COMPLETE

Goal: a clean Python module (or few small modules) that exposes a set of **feature functions** over grids, using standard libs, with minimal reinvented algorithms.

Refer to docs/WOs/M1/M1.md  for work orders and docs/WOs/M1/ to see detailed work orders 
---

## M2 â€“ indexing + ConstraintBuilder + SchemaFamily metadata + dispatch skeleton âœ… COMPLETE

High-level Work Orders in docs/WOs/M2/M2.md 
for detailed WOs refer to docs/WOs/M2/

---

## M3 â€“ Schema builders S1â€“S11 âœ… COMPLETE
* **M3** = actually make S1â€“S11 *do something* using M1+M2.
High level work orders in docs/WOs/M3/M3.md . refer to docs/WOs/M3/ for detailed work orders

---

## M4 â€“ Solver + Decoding + Validation âœ… COMPLETE
M4 is basically:
> **â€œTake constraints â†’ run an LP/ILP â†’ get y â†’ decode to Grid(s) â†’ check if it makes sense.â€**

High-level Work Orders in docs/WOs/M4/M4.md . detailed WOs in docs/WOs/M4/

---

## M5 â€“  Pi-agent interface, diagnostics, catalog building
M5 is where we make the **kernel â€œtalkâ€** â€” fail-closed, with rich, structured intermediate info that a Pi-agent can actually use to debug and refine laws.

Weâ€™re done with the math engine; now weâ€™re building the **interface and diagnostics layer** that a Pi-agent will sit on.

High-level Work Orders
### ðŸ”¹ WO-M5.1 â€“ Result & diagnostics struct âœ… COMPLETE

**Goal:** define a single, structured object that captures *everything* about a solve attempt, especially in failure.

**File:** `src/runners/results.py`

**Scope (high-level):**

* Define something like:

  ```python
  @dataclass
  class SolveDiagnostics:
      task_id: str
      law_config: TaskLawConfig
      status: Literal["ok", "infeasible", "mismatch", "error"]
      solver_status: str                 # from pulp
      num_constraints: int
      num_variables: int
      schema_ids_used: list[str]
      # optional: per-schema constraint counts

      # Only for training tasks:
      train_mismatches: list[dict]       # e.g. [{ "example_idx": 0, "diff_cells": [...] }, ...]

      # Debug:
      error_message: str | None
  ```

* This struct is what the Pi-agent will see:

  * if `status != "ok"`, it gets **explicit reasons**: infeasible, mismatch, where mismatched, etc.

---

### ðŸ”¹ WO-M5.2 â€“ Extend kernel to return diagnostics, not just grids âœ… COMPLETE

**Goal:** make `solve_arc_task` produce **diagnostics + outputs** in a way that is fail-closed and Pi-agent-friendly.

**File:** `src/runners/kernel.py` (augment)

**Scope:**

* Change / wrap `solve_arc_task(task_id, law_config)` to something like:

  ```python
  def solve_arc_task_with_diagnostics(
      task_id: str,
      law_config: TaskLawConfig,
      use_training_labels: bool = False
  ) -> tuple[dict[str, list[Grid]], SolveDiagnostics]:
      """
      Returns:
        outputs: {"train": [...], "test": [...]}
        diagnostics: SolveDiagnostics
      """
  ```

* Behavior:

  * Always:

    * build TaskContext,
    * build constraints via schemas,
    * call solver,
    * decode y â†’ grids,
    * fill `SolveDiagnostics` with:

      * status = "ok" / "infeasible" / "error".
  * If `use_training_labels=True`:

    * compare predicted vs true train outputs,
    * set status = "mismatch" if any differ,
    * populate `train_mismatches` with per-example diff info.

This is exactly the â€œfail-close + intermediate infoâ€ you mentioned.

---

### ðŸ”¹ WO-M5.3 â€“ Training sweep + catalog builder script

**Goal:** one script that a Pi-agent / human can drive to **try law configs on all training tasks and build/update the Catalog**.

**File:** `src/runners/build_catalog_from_training.py`

**Scope:**

* For each `task_id` in `arc-agi_training_challenges.json`:

  * Load an existing `TaskLawConfig` (if any) from `catalog/store.py` **or** receive one from outside (Pi-agent).
  * Call `solve_arc_task_with_diagnostics(task_id, law_config, use_training_labels=True)`.
  * If `status == "ok"`:

    * mark this config as **valid** for that task,
    * write/update it in the catalog store.
  * If `status == "mismatch"` or `"infeasible"`:

    * log diagnostics to a JSON or log file for that task:

      * mismatches, solver_status, schemas used.
* This script does **no law discovery**; it just:

  * runs the kernel,
  * records successes,
  * outputs failures in a Pi-agent-readable format.

This is the main â€œsweep + recordâ€ entrypoint.

---

### ðŸ”¹ WO-M5.4 â€“ Pi-agent harness / interface

**Goal:** a thin Python interface that exposes everything a Pi-agent needs via simple function calls, without forcing it to touch internals.

**File:** `src/agents/pi_interface.py`

**Scope:**

* Define functions like:

  ```python
  def load_task_summary(task_id: str) -> dict:
      """
      Returns:
        - basic info about the task,
        - maybe small grid previews,
        - counts of components/colors, etc.
      """

  def try_law_config_on_task(
      task_id: str,
      law_config: TaskLawConfig
  ) -> SolveDiagnostics:
      """
      Runs solve_arc_task_with_diagnostics(use_training_labels=True)
      but only returns diagnostics so Pi-agent can see status & mismatches.
      """

  def save_law_config(task_id: str, law_config: TaskLawConfig) -> None:
      """
      Writes to catalog via catalog.store, for configs Pi-agent believes are good.
      """
  ```

* The idea:

  * Pi-agent (you + LLM in an interactive session) only talk to `pi_interface`:

    * see task summaries,
    * propose/update law configs,
    * get rich diagnostics when it fails.

This layer is where we later plug â€œPi-agent as a prompt/programâ€ without touching the math kernel.

---

### ðŸ”¹ WO-M5.5 â€“ Human/Pi-agent log-friendly formatting

**Goal:** make failure cases easy to read and reason about (for both humans and Pi-agent).

**File:** `src/runners/logging_utils.py`

**Scope:**

* Helpers to pretty-print `SolveDiagnostics`:

  * print which schemas used,
  * print mismatched train examples with side-by-side grids,
  * print counts (constraints, variables).
* Optionally, convert `SolveDiagnostics` into a **compact JSON** that Pi-agent can read and reason about.

This is mostly sugar, but itâ€™s important for your â€œobserver is observedâ€ workflow: we want the system itself to provide introspectable state.

---

## ðŸ”¹ WO-M5.X â€“ Enrich diagnostics with per-schema stats & example summaries

**Goal:** give the Pi-agent more structured evidence by:

1. Tracking how many constraints each schema family contributed.
2. Providing a compact summary of each training/test example (shapes, components).

**Files touched (high-level):**

* `src/runners/results.py`
* `src/schemas/dispatch.py`
* `src/schemas/context.py` (or a new `summaries.py`)
* `src/runners/kernel.py`

---

### Part A â€“ Per-schema constraint counts

**Idea:** each time we apply a schema instance, record how many constraints it added.

**High-level scope:**

* Extend `SolveDiagnostics` with:

  ```python
  schema_constraint_counts: Dict[str, int]
  ```

* In `apply_schema_instance(...)` (in `dispatch.py`):

  * Capture `before = len(builder.constraints)`.
  * Call the `build_Sk_constraints(...)` function.
  * Capture `after = len(builder.constraints)`.
  * Increment `schema_constraint_counts[family_id] += (after - before)`.

* Ensure `kernel.solve_arc_task_with_diagnostics(...)` passes this dict into `SolveDiagnostics`.

**Outcome:** Pi-agent can see which schemas were *actually active* and how heavily they were used.

---

### Part B â€“ Example-level summary (compact)

**Idea:** each example (train/test) gets a small digest: shape + component stats.

**High-level scope:**

* Define an `ExampleSummary` dataclass, e.g. in `results.py` or a new `summaries.py`:

  ```python
  @dataclass
  class ExampleSummary:
      input_shape: tuple[int, int]
      output_shape: Optional[tuple[int, int]]  # None for test inputs
      components_per_color: Dict[int, int]     # color -> count of components
  ```

* Extend `SolveDiagnostics` with:

  ```python
  example_summaries: List[ExampleSummary]
  ```

* In `kernel.solve_arc_task_with_diagnostics(...)`:

  * For each ExampleContext in `TaskContext.train_examples` and `test_examples`:

    * Compute `input_shape`, `output_shape` from grids.
    * Use `components` to build `components_per_color` (count map).
    * Append an `ExampleSummary` to a list.
  * Pass that list into `SolveDiagnostics`.

**Outcome:** Pi-agent can quickly see:

* per-example sizes,
* whether output shape matches expectation,
* how many components of each color exist.
---

### How this ties together

After M1 we have:

* A **grid IO + indexing module**.
* A **feature module** that can compute:

  * coordinates and bands,
  * border flags,
  * connected components + shape signatures,
  * object_id per pixel,
  * row/col nonzero flags,
  * neighborhood hashes.

This is Ï†(p) in code.

* **M1** = Ï† + IO 
* **M2** = indexing + ConstraintBuilder + SchemaFamily metadata + dispatch skeleton

From there, we can:

* Define the **SchemaFamily registry** and **builder functions** (next milestones).
* Then hook ConstraintBuilder + solver.
* Finally, bring in the Pi-agent that uses all this.
